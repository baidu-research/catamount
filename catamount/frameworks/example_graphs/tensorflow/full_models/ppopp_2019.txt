Principles and Practice of Parallel Programming (PPoPP) 2019:

In addition to providing full-graph tests, this directory contains
reproduceable results for the PPoPP paper: J. Hestness, N. Ardalani, G.
Diamos, Beyond Human-Level Accuracy: Computational Challenges in Deep
Learning, PPoPP Feb. 2019.

The .meta files in this directory were created by Tensorflow training runs of
state-of-the-art models in order to calculate their compute and memory
requirements. The following are short descriptions of each of the models (in
case you'd rather not load them and try to dissect their compute graphs ;).


Character Language Model (language_models/char_lm_n2004_l10_sgd_lr0.15_rhn_b128_vchar_d1.0_s150-latest_model.meta)
This model is a recurrent highway network (RHN) model with a single RHN layer
containing 10 sublayers ('depth 10'). Described in Zilly et al. Recurrent
Highway Networks. In Proceedings of The International Conference on Machine
Learning (ICML), 2017. 


Image Classification with ResNets (image_classification/graph_d*_fs1.0_bs32.meta)
ResNet models are used as the baseline image classification networks, and we
include 5 popular model sizes (depths). Specifically, the image_classification
directory includes ResNets 18, 34, 50, 101, and 152. These networks are deep
convolutional networks with pooling and skip connections. While ResNet 18 and
34 use the basic-block architecture, ResNet 50, 101, and 152 use the
bottleneck architecture. These models are described in He et al., Deep
Residual Learning for Image Recognition. IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016. 770–778.


Machine Translation (language_models/nmt_el2_dl1_n1024_b128-translate.ckpt-1000.meta)
The neural machine translation model is similar to Google's NMT model
(https://github.com/tensorflow/nmt), which is an encoder-decoder model with
an attention mechanism. We use the attention mechanism described in
Luong et al., Effective Approaches to Attention-based Neural Machine
Translation. Empirical Methods in Natural Language Processing (EMNLP), 2015.
1412–1421.


Speech Attention (speech_attention/model.ckpt.meta):
This model is the hybrid attention model described in Battenberg et al.,
Exploring Neural Transducers for End-to-End Speech Recognition, IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU) 2017. The model has an
encoder comprised of three bidirectional LSTM layers with two intermediate
max-pooling layers, and a hybrid attention decoder.


Word Language Model (language_models/word_lm_n2004_l2_sgd_lr0.2_nodrop_b128_v10k_d20_s80-best_model.meta)
The WordLM is a two-layer LSTM model with embedding and output layers to
relatively large vocabulary of 10,000 words. Model is described in Jozefowicz
et al. Exploring the Limits of Language Modeling. arXiv preprint
arXiv:1602.02410v2, 2016. 
